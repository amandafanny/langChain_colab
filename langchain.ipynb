{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QtQ4c_zVpmuZ"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNenDo/s2wjnPyQ7MV7oPlt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amandafanny/langChain_colab/blob/main/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 安装环境\n",
        "OPENAI_API_KEY = \"\" #@param {type:\"string\"}\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "\n",
        "import os\n",
        "%env\n",
        "# %reload_ext autoreload\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"\""
      ],
      "metadata": {
        "id": "2ZigKsbSeYKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "9fD5Y-28rX-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs"
      ],
      "metadata": {
        "id": "Sqr_K2XMrrSh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRugXlq6eHlD"
      },
      "outputs": [],
      "source": [
        "#@title 异步处理\n",
        "import time\n",
        "import asyncio\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "def generate_serially():\n",
        "    llm = OpenAI(temperature=0.9)\n",
        "    for _ in range(10):\n",
        "        resp = llm.generate([\"Hello, how are you?\"])\n",
        "        print(resp.generations[0][0].text)\n",
        "\n",
        "\n",
        "async def async_generate(llm):\n",
        "    resp = await llm.agenerate([\"Hello, how are you?\"])\n",
        "    print(resp.generations[0][0].text)\n",
        "\n",
        "\n",
        "async def generate_concurrently():\n",
        "    llm = OpenAI(temperature=0.9)\n",
        "    tasks = [async_generate(llm) for _ in range(10)]\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "\n",
        "s = time.perf_counter()\n",
        "# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n",
        "await generate_concurrently() \n",
        "elapsed = time.perf_counter() - s\n",
        "print('\\033[1m' + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + '\\033[0m')\n",
        "\n",
        "s = time.perf_counter()\n",
        "generate_serially()\n",
        "elapsed = time.perf_counter() - s\n",
        "print('\\033[1m' + f\"Serial executed in {elapsed:0.2f} seconds.\" + '\\033[0m')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title How to write a custom LLM wrappe\n",
        "\n",
        "#@markdown There is only one required thing that a custom LLM needs to implement:\n",
        "\n",
        "#@markdown 1. A `_call` method that takes in a string, some optional stop words, and returns a string\n",
        "\n",
        "#@markdown There is a second optional thing it can implement:\n",
        "\n",
        "#@markdown 1. An `_identifying_params` property that is used to help with printing of this class. Should return a dictionary.\n",
        "\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Mapping, Any\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "    \n",
        "    n: int\n",
        "        \n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "    \n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        if stop is not None:\n",
        "            raise ValueError(\"stop kwargs are not permitted.\")\n",
        "        return prompt[:self.n]\n",
        "    \n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "        return {\"n\": self.n}\n",
        "\n",
        "llm = CustomLLM(n=5)\n",
        "print(llm(\"This is a foobar thing\"))\n",
        "print(llm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "US6sAB5LjMP8",
        "outputId": "4c55b92f-36f4-45a4-a13e-7c501f827984"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mCustomLLM\u001b[0m\n",
            "Params: {'n': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.fake import FakeListLLM\n",
        "\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "tools = load_tools([\"python_repl\"])\n",
        "\n",
        "responses=[\n",
        "    \"Action: Python REPL\\nAction Input: print(2 + 2)\",\n",
        "    \"Final Answer: 4\"\n",
        "]\n",
        "llm = FakeListLLM(responses=responses)\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "agent.run(\"whats 2 + 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "R0sqwNxXjyNj",
        "outputId": "b8a369f1-2019-4f57-dbe3-408d5884a6fa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
            "Action Input: print(2 + 2)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m4\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to cache LLM calls"
      ],
      "metadata": {
        "id": "6e2ysC0EqXqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In Memory Cache"
      ],
      "metadata": {
        "id": "QtQ4c_zVpmuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "\n",
        "# To make the caching really obvious, lets use a slower model.\n",
        "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
      ],
      "metadata": {
        "id": "y7kDdbLNptCZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm(\"Tell me a joke\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "ulya1xiAo6AK",
        "outputId": "9de60d16-3f6e-4743-9a44-961317edc302"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.8 ms, sys: 0 ns, total: 12.8 ms\n",
            "Wall time: 876 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nTwo guys stole a calendar. They got six months each.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "llm(\"Tell me a joke\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "YuLfcDRvomi-",
        "outputId": "6f111725-4cc3-4285-db19-b758e2cd3ebc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 194 µs, sys: 0 ns, total: 194 µs\n",
            "Wall time: 200 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nTwo guys stole a calendar. They got six months each.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SQLite"
      ],
      "metadata": {
        "id": "2X4T8nWRpY9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm .langchain.db\n",
        "\n",
        "# We can do the same thing with a SQLite cache\n",
        "from langchain.cache import SQLiteCache\n",
        "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n"
      ],
      "metadata": {
        "id": "MOsKnc1rpHl2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm(\"Tell me a joke\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "DAXHwxTdpXNC",
        "outputId": "d86d6aed-e374-4333-ffe2-670e7e39faaf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22.1 ms, sys: 3.65 ms, total: 25.8 ms\n",
            "Wall time: 1.39 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "llm(\"Tell me a joke\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "gLTxYK6Tqt8S",
        "outputId": "cd26ae1a-9602-4381-d37c-511a80a92035"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.36 ms, sys: 973 µs, total: 2.33 ms\n",
            "Wall time: 2.34 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Redis Cache"
      ],
      "metadata": {
        "id": "M-md_QBJrLSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install redis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9hPNFYPsv9B",
        "outputId": "e758c20c-2080-4baa-c110-f1f6e8cd6005"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting redis\n",
            "  Downloading redis-4.5.4-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.9/dist-packages (from redis) (4.0.2)\n",
            "Installing collected packages: redis\n",
            "Successfully installed redis-4.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can do the same thing with a Redis cache\n",
        "# (make sure your local Redis instance is running first before running this example)\n",
        "from redis import Redis\n",
        "from langchain.cache import RedisCache\n",
        "langchain.llm_cache = RedisCache(redis_=Redis())"
      ],
      "metadata": {
        "id": "AfKmikVqsacp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm(\"Tell me a joke\")"
      ],
      "metadata": {
        "id": "WYcOoYENs5Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "llm(\"Tell me a joke\")"
      ],
      "metadata": {
        "id": "5NIhDAEMs7zK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}